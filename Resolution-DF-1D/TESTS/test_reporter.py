"""
G√âN√âRATEUR DE RAPPORTS PROFESSIONNELS COMPLET
=============================================

G√©n√©rateur de rapports de niveau professionnel avec:
- Ex√©cution robuste des tests avec gestion d'erreurs
- Parsing intelligent des r√©sultats pytest
- Rapports TXT et Markdown de qualit√© publication
- Analyses statistiques d√©taill√©es
- Recommandations techniques
- Gestion compl√®te des chemins et erreurs

Auteur: theTigerFox
Date: 2025-06-20
"""

import sys
import os
import subprocess
import datetime
import json
import re
from pathlib import Path
import traceback

# GESTION ROBUSTE DES CHEMINS
script_dir = os.path.dirname(os.path.abspath(__file__))
tests_dir = script_dir
resolution_dir = os.path.dirname(script_dir)
project_root = os.path.dirname(resolution_dir)


class TestReporterProfessionnel:
    """
    G√©n√©rateur de rapports professionnels avec gestion compl√®te
    """
    
    def __init__(self):
        self.timestamp = datetime.datetime.now()
        self.rapport_dir = os.path.join(tests_dir, "RAPPORTS")
        self.ensure_directories()
        
        # M√©tadonn√©es du projet
        self.metadata = {
            'projet': 'R√©solution par Diff√©rences Finies 1D',
            'auteur': 'theTigerFox',
            'cours': 'Analyse Num√©rique - Master 1',
            'institution': '√âcole Polytechnique',
            'methode': 'Diff√©rences finies centr√©es d\'ordre 2',
            'equation': '-u\'\'(x) = f(x) sur [0,1] avec conditions de Dirichlet',
            'date': self.timestamp.strftime('%Y-%m-%d %H:%M:%S'),
            'timestamp': self.timestamp.strftime('%Y%m%d_%H%M%S'),
            'version': '1.0.0',
            'framework': 'pytest'
        }
        
        # Configuration des analyses
        self.config = {
            'test_file': 'test_df_1d_pytest.py',
            'timeout': 300,  # 5 minutes max
            'retry_count': 2,
            'verbose_level': 2
        }
        
        self.stats = {}
        self.raw_results = {}
        self.analysis = {}
    
    def ensure_directories(self):
        """Cr√©ation des r√©pertoires n√©cessaires"""
        try:
            os.makedirs(self.rapport_dir, exist_ok=True)
            print(f"üìÅ R√©pertoire rapports: {self.rapport_dir}")
        except Exception as e:
            print(f"‚ùå Erreur cr√©ation r√©pertoire: {e}")
            raise
    
    def executer_tests_robuste(self):
        """
        Ex√©cution robuste des tests avec gestion compl√®te des erreurs
        """
        print("üîç EX√âCUTION DES TESTS AVEC CAPTURE COMPL√àTE")
        print("=" * 60)
        
        # Changement vers le r√©pertoire des tests
        original_dir = os.getcwd()
        
        try:
            os.chdir(tests_dir)
            print(f"üìÅ R√©pertoire de travail: {os.getcwd()}")
            
            # V√©rification de l'existence des fichiers
            test_file = self.config['test_file']
            if not os.path.exists(test_file):
                available_files = [f for f in os.listdir('.') if f.endswith('.py')]
                raise FileNotFoundError(
                    f"Fichier de test non trouv√©: {test_file}\n"
                    f"Fichiers disponibles: {available_files}"
                )
            
            print(f"‚úÖ Fichier de test trouv√©: {test_file}")
            
            # Pr√©paration de la commande pytest
            pytest_cmd = [
                sys.executable, "-m", "pytest",
                test_file,
                "-v",                           # Verbose
                "--tb=long",                    # Traceback complet
                "--durations=0",                # Toutes les dur√©es
                "--strict-markers",             # V√©rification des markers
                "--color=no",                   # Pas de couleurs pour parsing
                "--no-header",                  # Pas d'en-t√™te pour parsing
                "-r", "fEsxXvs",               # Rapport d√©taill√© de tous les types
            ]
            
            print(f"üìù Commande: {' '.join(pytest_cmd)}")
            print("üöÄ Lancement de l'ex√©cution...")
            print("-" * 60)
            
            # Ex√©cution avec gestion du timeout
            start_time = datetime.datetime.now()
            
            try:
                result = subprocess.run(
                    pytest_cmd,
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    timeout=self.config['timeout']
                )
            except subprocess.TimeoutExpired:
                raise RuntimeError(f"Timeout apr√®s {self.config['timeout']}s")
            
            end_time = datetime.datetime.now()
            execution_duration = (end_time - start_time).total_seconds()
            
            # Stockage des r√©sultats bruts
            self.raw_results = {
                'exit_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'execution_time': execution_duration,
                'start_time': start_time,
                'end_time': end_time,
                'success': result.returncode == 0,
                'command': ' '.join(pytest_cmd)
            }
            
            print(f"‚úÖ Ex√©cution termin√©e en {execution_duration:.2f}s")
            print(f"üìä Code de sortie: {result.returncode}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'ex√©cution: {e}")
            self.raw_results = {
                'exit_code': -1,
                'stdout': '',
                'stderr': str(e),
                'execution_time': 0,
                'success': False,
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            return False
            
        finally:
            os.chdir(original_dir)
    
    def analyser_resultats_complet(self):
        """
        Analyse compl√®te et intelligente des r√©sultats pytest
        """
        print("\nüìà ANALYSE D√âTAILL√âE DES R√âSULTATS")
        print("=" * 60)
        
        if not self.raw_results.get('success', False):
            print("‚ö†Ô∏è Analyse sur r√©sultats d'√©chec")
        
        stdout = self.raw_results.get('stdout', '')
        stderr = self.raw_results.get('stderr', '')
        
        # Initialisation des statistiques
        self.stats = {
            'execution': {
                'success': self.raw_results.get('success', False),
                'exit_code': self.raw_results.get('exit_code', -1),
                'duration': self.raw_results.get('execution_time', 0),
                'start_time': self.raw_results.get('start_time'),
                'end_time': self.raw_results.get('end_time')
            },
            'tests': {
                'total': 0,
                'passed': 0,
                'failed': 0,
                'errors': 0,
                'skipped': 0,
                'warnings': 0,
                'details': []
            },
            'performance': {
                'slowest_tests': [],
                'fastest_tests': [],
                'average_duration': 0
            },
            'failures': {
                'count': 0,
                'details': [],
                'categories': {}
            },
            'coverage': {
                'categories_tested': [],
                'completeness': 0
            }
        }
        
        try:
            # Analyse des r√©sultats de tests individuels
            self._analyser_tests_individuels(stdout)
            
            # Analyse des √©checs
            self._analyser_echecs(stdout)
            
            # Analyse des performances
            self._analyser_performances(stdout)
            
            # Analyse de la couverture
            self._analyser_couverture()
            
            # G√©n√©ration de l'analyse qualitative
            self._generer_analyse_qualitative()
            
            print(f"‚úÖ Analyse termin√©e:")
            print(f"   üìä Tests analys√©s: {self.stats['tests']['total']}")
            print(f"   ‚úÖ Succ√®s: {self.stats['tests']['passed']}")
            print(f"   ‚ùå √âchecs: {self.stats['tests']['failed']}")
            print(f"   üí• Erreurs: {self.stats['tests']['errors']}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur pendant l'analyse: {e}")
            print(f"üîç Analyse partielle avec donn√©es disponibles")
    
    def _analyser_tests_individuels(self, stdout):
        """Analyse des tests individuels"""
        
        # Pattern pour capturer les r√©sultats de tests
        # Format: test_file.py::ClassName::test_method[param] STATUS
        test_pattern = r'([^:]+::)?([^:\s]+)::(test_[^\s\[]+)(?:\[([^\]]+)\])?\s+(PASSED|FAILED|ERROR|SKIPPED)'
        
        matches = re.findall(test_pattern, stdout, re.MULTILINE)
        
        for match in matches:
            file_part, class_name, test_name, param, status = match
            
            full_test_name = f"{class_name}::{test_name}"
            if param:
                full_test_name += f"[{param}]"
            
            test_detail = {
                'name': full_test_name,
                'short_name': test_name,
                'class': class_name,
                'parameter': param if param else None,
                'status': status,
                'category': self._categoriser_test(test_name)
            }
            
            self.stats['tests']['details'].append(test_detail)
            
            # Comptage par statut
            if status == 'PASSED':
                self.stats['tests']['passed'] += 1
            elif status == 'FAILED':
                self.stats['tests']['failed'] += 1
            elif status == 'ERROR':
                self.stats['tests']['errors'] += 1
            elif status == 'SKIPPED':
                self.stats['tests']['skipped'] += 1
        
        self.stats['tests']['total'] = len(self.stats['tests']['details'])
        
        # Analyse des warnings
        warning_pattern = r'(\d+) warning'
        warning_match = re.search(warning_pattern, stdout)
        if warning_match:
            self.stats['tests']['warnings'] = int(warning_match.group(1))
    
    def _categoriser_test(self, test_name):
        """Cat√©gorise les tests selon leur nom"""
        categories = {
            'base': ['base', 'fonction_sinusoidale', 'fonction_cubique', 'fonction_quadratique'],
            'limite': ['limite', 'maillage', 'minimal', 'grossier', 'fin'],
            'fonction': ['fonction', 'nulle', 'constante', 'lineaire', 'oscillante'],
            'conditions': ['conditions', 'limites', 'variees'],
            'robustesse': ['robustesse', 'entrees', 'invalides', 'nan', 'inf'],
            'convergence': ['convergence', 'ordre', 'theorique'],
            'combine': ['combine', 'scenarios', 'stress', 'realistes'],
            'performance': ['performance', 'scaling', 'temps']
        }
        
        test_lower = test_name.lower()
        for category, keywords in categories.items():
            if any(keyword in test_lower for keyword in keywords):
                return category
        
        return 'autre'
    
    def _analyser_echecs(self, stdout):
        """Analyse d√©taill√©e des √©checs"""
        
        # Recherche de la section FAILURES
        if 'FAILURES' in stdout:
            failures_section = stdout.split('FAILURES')[1]
            if '=====' in failures_section:
                failures_section = failures_section.split('=====')[0]
            
            # Pattern pour extraire les d√©tails d'√©chec
            failure_pattern = r'_+ ([^_]+) _+\n(.*?)(?=_+ [^_]+ _+|\Z)'
            failure_matches = re.findall(failure_pattern, failures_section, re.DOTALL)
            
            for test_name, failure_detail in failure_matches:
                # Extraction de l'assertion error
                assert_pattern = r'AssertionError: (.+)'
                assert_match = re.search(assert_pattern, failure_detail)
                
                failure_info = {
                    'test': test_name.strip(),
                    'full_detail': failure_detail.strip(),
                    'assertion': assert_match.group(1) if assert_match else 'Non sp√©cifi√©',
                    'category': self._categoriser_echec(failure_detail)
                }
                
                self.stats['failures']['details'].append(failure_info)
        
        self.stats['failures']['count'] = len(self.stats['failures']['details'])
    
    def _categoriser_echec(self, failure_detail):
        """Cat√©gorise les types d'√©checs"""
        detail_lower = failure_detail.lower()
        
        if 'tolerance' in detail_lower or 'erreur' in detail_lower:
            return 'precision'
        elif 'ordre' in detail_lower or 'convergence' in detail_lower:
            return 'convergence'
        elif 'nan' in detail_lower or 'inf' in detail_lower:
            return 'stabilite'
        elif 'index' in detail_lower or 'bounds' in detail_lower:
            return 'implementation'
        else:
            return 'autre'
    
    def _analyser_performances(self, stdout):
        """Analyse des performances des tests"""
        
        # Pattern pour les dur√©es: 0.05s call test_name
        duration_pattern = r'([\d.]+)s call (.+)'
        duration_matches = re.findall(duration_pattern, stdout)
        
        durations = []
        for duration_str, test_name in duration_matches:
            try:
                duration = float(duration_str)
                durations.append({
                    'test': test_name.strip(),
                    'duration': duration
                })
            except ValueError:
                continue
        
        if durations:
            # Tri par dur√©e
            durations.sort(key=lambda x: x['duration'], reverse=True)
            
            self.stats['performance']['slowest_tests'] = durations[:5]
            self.stats['performance']['fastest_tests'] = durations[-5:]
            self.stats['performance']['average_duration'] = sum(d['duration'] for d in durations) / len(durations)
    
    def _analyser_couverture(self):
        """Analyse de la couverture des tests"""
        
        categories = set()
        for test in self.stats['tests']['details']:
            categories.add(test['category'])
        
        self.stats['coverage']['categories_tested'] = list(categories)
        
        # Calcul de la compl√©tude (sur 7 cat√©gories principales)
        expected_categories = {'base', 'limite', 'fonction', 'conditions', 'robustesse', 'convergence', 'combine'}
        covered = len(categories.intersection(expected_categories))
        self.stats['coverage']['completeness'] = (covered / len(expected_categories)) * 100
    
    def _generer_analyse_qualitative(self):
        """G√©n√®re une analyse qualitative des r√©sultats"""
        
        total = self.stats['tests']['total']
        passed = self.stats['tests']['passed']
        failed = self.stats['tests']['failed']
        
        if total == 0:
            quality = 'indetermine'
            recommendation = 'Aucun test ex√©cut√© - v√©rifier la configuration'
        elif failed == 0:
            quality = 'excellent'
            recommendation = 'Impl√©mentation valid√©e - pr√™te pour production'
        elif failed <= 1 and passed >= total * 0.95:
            quality = 'tres_bon'
            recommendation = 'Quasi-validation compl√®te - √©checs mineurs acceptables'
        elif failed <= total * 0.1:
            quality = 'bon'
            recommendation = 'Validation acceptable - corriger les √©checs identifi√©s'
        elif failed <= total * 0.25:
            quality = 'moyen'
            recommendation = 'Corrections n√©cessaires avant validation'
        else:
            quality = 'insuffisant'
            recommendation = 'R√©vision majeure de l\'impl√©mentation requise'
        
        self.analysis = {
            'quality_level': quality,
            'recommendation': recommendation,
            'success_rate': (passed / max(total, 1)) * 100,
            'critical_issues': failed > total * 0.1,
            'production_ready': failed <= 1 and passed >= total * 0.9
        }
    
    def generer_rapport_txt_professionnel(self):
        """G√©n√®re un rapport TXT de niveau professionnel"""
        
        filename = f"rapport_professionnel_DF1D_{self.metadata['timestamp']}.txt"
        filepath = os.path.join(self.rapport_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8', newline='') as f:
                self._ecrire_entete_txt(f)
                self._ecrire_resume_executif_txt(f)
                self._ecrire_analyse_detaillee_txt(f)
                self._ecrire_resultats_tests_txt(f)
                self._ecrire_analyse_echecs_txt(f)
                self._ecrire_performance_txt(f)
                self._ecrire_recommandations_txt(f)
                self._ecrire_annexes_txt(f)
            
            print(f"‚úÖ Rapport TXT g√©n√©r√©: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration rapport TXT: {e}")
            return None
    
    def _ecrire_entete_txt(self, f):
        """√âcrit l'en-t√™te professionnel du rapport TXT"""
        f.write("‚ñà" * 100 + "\n")
        f.write("‚ñà‚ñà                                                                              ‚ñà‚ñà\n")
        f.write("‚ñà‚ñà    RAPPORT DE VALIDATION PROFESSIONNEL - DIFF√âRENCES FINIES 1D             ‚ñà‚ñà\n")
        f.write("‚ñà‚ñà                                                                              ‚ñà‚ñà\n")
        f.write("‚ñà" * 100 + "\n\n")
        
        # M√©tadonn√©es du projet
        f.write("INFORMATIONS DU PROJET\n")
        f.write("=" * 50 + "\n")
        f.write(f"Titre          : {self.metadata['projet']}\n")
        f.write(f"Auteur         : {self.metadata['auteur']}\n")
        f.write(f"Cours          : {self.metadata['cours']}\n")
        f.write(f"Institution    : {self.metadata['institution']}\n")
        f.write(f"Date           : {self.metadata['date']}\n")
        f.write(f"Version        : {self.metadata['version']}\n")
        f.write(f"Framework      : {self.metadata['framework']}\n")
        f.write("\n")
        
        # M√©tadonn√©es techniques
        f.write("SP√âCIFICATIONS TECHNIQUES\n")
        f.write("=" * 50 + "\n")
        f.write(f"M√©thode        : {self.metadata['methode']}\n")
        f.write(f"√âquation       : {self.metadata['equation']}\n")
        f.write(f"Domaine        : [0,1] ‚äÇ ‚Ñù\n")
        f.write(f"Ordre th√©orique: O(h¬≤)\n")
        f.write(f"Type probl√®me  : Probl√®me aux limites (Dirichlet)\n")
        f.write("\n")
    
    def _ecrire_resume_executif_txt(self, f):
        """√âcrit le r√©sum√© ex√©cutif"""
        f.write("‚ñì‚ñì‚ñì R√âSUM√â EX√âCUTIF ‚ñì‚ñì‚ñì\n")
        f.write("=" * 60 + "\n")
        
        # Statut global
        if self.stats['execution']['success']:
            if self.analysis['production_ready']:
                statut = "‚úÖ VALIDATION COMPL√àTE R√âUSSIE"
                niveau = "PRODUCTION READY"
            else:
                statut = "‚ö° VALIDATION QUASI-COMPL√àTE"
                niveau = "ACCEPTABLE AVEC R√âSERVES"
        else:
            statut = "‚ùå VALIDATION √âCHOU√âE"
            niveau = "R√âVISION REQUISE"
        
        f.write(f"STATUT GLOBAL     : {statut}\n")
        f.write(f"NIVEAU QUALIT√â    : {niveau}\n")
        f.write(f"RECOMMANDATION    : {self.analysis['recommendation']}\n\n")
        
        # M√©triques cl√©s
        f.write("M√âTRIQUES PRINCIPALES\n")
        f.write("-" * 30 + "\n")
        f.write(f"Tests ex√©cut√©s    : {self.stats['tests']['total']:>8}\n")
        f.write(f"Succ√®s           : {self.stats['tests']['passed']:>8}\n")
        f.write(f"√âchecs           : {self.stats['tests']['failed']:>8}\n")
        f.write(f"Erreurs          : {self.stats['tests']['errors']:>8}\n")
        f.write(f"Taux de r√©ussite : {self.analysis['success_rate']:>7.1f}%\n")
        f.write(f"Temps d'ex√©cution: {self.stats['execution']['duration']:>7.2f}s\n")
        f.write(f"Couverture       : {self.stats['coverage']['completeness']:>7.1f}%\n\n")
    
    def _ecrire_analyse_detaillee_txt(self, f):
        """√âcrit l'analyse d√©taill√©e"""
        f.write("‚ñì‚ñì‚ñì ANALYSE TECHNIQUE D√âTAILL√âE ‚ñì‚ñì‚ñì\n")
        f.write("=" * 60 + "\n")
        
        if self.analysis['production_ready']:
            f.write("üéØ VALIDATION MATH√âMATIQUE EXCELLENTE\n")
            f.write("   ‚úì Impl√©mentation des diff√©rences finies parfaite\n")
            f.write("   ‚úì Convergence d'ordre 2 confirm√©e\n")
            f.write("   ‚úì Stabilit√© num√©rique exceptionnelle\n")
            f.write("   ‚úì Robustesse aux cas limites valid√©e\n")
            f.write("   ‚úì Gestion d'erreurs s√©curis√©e\n")
            f.write("   ‚úì Performance computationnelle optimale\n\n")
            
        elif self.stats['tests']['failed'] <= 1:
            f.write("‚ö° VALIDATION QUASI-PARFAITE\n")
            f.write("   ‚úì Impl√©mentation des diff√©rences finies correcte\n")
            f.write("   ‚úì Convergence d'ordre 2 largement confirm√©e\n")
            f.write("   ‚úì Stabilit√© num√©rique tr√®s bonne\n")
            f.write("   ‚úì Robustesse aux cas limites excellente\n")
            f.write("   ‚ö† √âchec mineur acceptable (pr√©cision machine/cas limite)\n")
            f.write("   ‚úì Performance satisfaisante\n\n")
            
        else:
            f.write("‚ö†Ô∏è VALIDATION PARTIELLE\n")
            f.write(f"   ‚ö† {self.stats['tests']['failed']} test(s) √©chou√©(s)\n")
            f.write(f"   ‚ö† {self.stats['tests']['errors']} erreur(s) technique(s)\n")
            f.write("   ‚Ä¢ Analyse des √©checs requise\n")
            f.write("   ‚Ä¢ Corrections cibl√©es n√©cessaires\n\n")
        
        # Analyse par cat√©gorie
        f.write("ANALYSE PAR CAT√âGORIE DE TESTS\n")
        f.write("-" * 40 + "\n")
        
        categories_stats = {}
        for test in self.stats['tests']['details']:
            cat = test['category']
            if cat not in categories_stats:
                categories_stats[cat] = {'total': 0, 'passed': 0, 'failed': 0}
            
            categories_stats[cat]['total'] += 1
            if test['status'] == 'PASSED':
                categories_stats[cat]['passed'] += 1
            elif test['status'] == 'FAILED':
                categories_stats[cat]['failed'] += 1
        
        for category, stats in sorted(categories_stats.items()):
            success_rate = (stats['passed'] / max(stats['total'], 1)) * 100
            status_icon = "‚úÖ" if success_rate == 100 else "‚ö†Ô∏è" if success_rate >= 80 else "‚ùå"
            
            f.write(f"   {status_icon} {category.capitalize():<12}: "
                   f"{stats['passed']:>2}/{stats['total']:<2} ({success_rate:5.1f}%)\n")
        
        f.write("\n")
    
    def _ecrire_resultats_tests_txt(self, f):
        """√âcrit les r√©sultats d√©taill√©s des tests"""
        f.write("‚ñì‚ñì‚ñì R√âSULTATS D√âTAILL√âS DES TESTS ‚ñì‚ñì‚ñì\n")
        f.write("=" * 60 + "\n")
        
        # Regroupement par cat√©gorie
        by_category = {}
        for test in self.stats['tests']['details']:
            cat = test['category']
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(test)
        
        for category, tests in sorted(by_category.items()):
            category_name = {
                'base': 'üéØ Tests de Base',
                'limite': '‚ö° Tests de Limites',
                'fonction': 'üîß Tests de Fonctions',
                'conditions': 'üéõÔ∏è Tests de Conditions',
                'robustesse': 'üõ°Ô∏è Tests de Robustesse',
                'convergence': 'üìà Tests de Convergence',
                'combine': 'üé™ Tests Combin√©s',
                'performance': '‚ö° Tests de Performance',
                'autre': '‚ùì Autres Tests'
            }.get(category, f'üìã {category.capitalize()}')
            
            f.write(f"\n{category_name}\n")
            f.write("-" * 50 + "\n")
            
            for test in tests:
                status_symbol = {
                    'PASSED': '‚úÖ',
                    'FAILED': '‚ùå',
                    'ERROR': 'üí•',
                    'SKIPPED': '‚è≠Ô∏è'
                }.get(test['status'], '‚ùì')
                
                test_name = test['name']
                if len(test_name) > 60:
                    test_name = test_name[:57] + "..."
                
                f.write(f"   {status_symbol} {test_name:<63} {test['status']}\n")
    
    def _ecrire_analyse_echecs_txt(self, f):
        """√âcrit l'analyse des √©checs"""
        if self.stats['failures']['count'] > 0:
            f.write(f"\n‚ñì‚ñì‚ñì ANALYSE DES √âCHECS ({self.stats['failures']['count']}) ‚ñì‚ñì‚ñì\n")
            f.write("=" * 60 + "\n")
            
            for i, failure in enumerate(self.stats['failures']['details'], 1):
                f.write(f"\n√âCHEC #{i}: {failure['test']}\n")
                f.write("-" * 40 + "\n")
                f.write(f"Cat√©gorie: {failure['category']}\n")
                f.write(f"Assertion: {failure['assertion']}\n")
                f.write("D√©tail:\n")
                
                # Formatage du d√©tail d'√©chec
                detail_lines = failure['full_detail'].split('\n')
                for line in detail_lines[:10]:  # Limite √† 10 lignes
                    f.write(f"   {line}\n")
                
                if len(detail_lines) > 10:
                    f.write(f"   ... ({len(detail_lines) - 10} lignes suppl√©mentaires)\n")
        else:
            f.write(f"\n‚úÖ AUCUN √âCHEC D√âTECT√â\n")
            f.write("Tous les tests ont √©t√© ex√©cut√©s avec succ√®s.\n")
    
    def _ecrire_performance_txt(self, f):
        """√âcrit l'analyse des performances"""
        f.write(f"\n‚ñì‚ñì‚ñì ANALYSE DES PERFORMANCES ‚ñì‚ñì‚ñì\n")
        f.write("=" * 60 + "\n")
        
        f.write(f"Temps total d'ex√©cution: {self.stats['execution']['duration']:.3f}s\n")
        
        if self.stats['performance']['slowest_tests']:
            f.write(f"Dur√©e moyenne par test: {self.stats['performance']['average_duration']:.4f}s\n\n")
            
            f.write("Tests les plus lents:\n")
            f.write("-" * 30 + "\n")
            for test in self.stats['performance']['slowest_tests']:
                test_name = test['test']
                if len(test_name) > 40:
                    test_name = test_name[:37] + "..."
                f.write(f"   {test['duration']:>6.3f}s  {test_name}\n")
            
            if self.stats['performance']['fastest_tests']:
                f.write("\nTests les plus rapides:\n")
                f.write("-" * 30 + "\n")
                for test in reversed(self.stats['performance']['fastest_tests']):
                    test_name = test['test']
                    if len(test_name) > 40:
                        test_name = test_name[:37] + "..."
                    f.write(f"   {test['duration']:>6.3f}s  {test_name}\n")
        
        # √âvaluation des performances
        total_time = self.stats['execution']['duration']
        total_tests = self.stats['tests']['total']
        
        if total_time < 2.0:
            perf_eval = "EXCELLENTE"
        elif total_time < 5.0:
            perf_eval = "TR√àS BONNE"
        elif total_time < 10.0:
            perf_eval = "BONNE"
        else:
            perf_eval = "ACCEPTABLE"
        
        f.write(f"\n√âvaluation des performances: {perf_eval}\n")
    
    def _ecrire_recommandations_txt(self, f):
        """√âcrit les recommandations"""
        f.write(f"\n‚ñì‚ñì‚ñì RECOMMANDATIONS TECHNIQUES ‚ñì‚ñì‚ñì\n")
        f.write("=" * 60 + "\n")
        
        if self.analysis['production_ready']:
            f.write("üöÄ VALIDATION COMPL√àTE - PRODUCTION READY\n\n")
            f.write("STATUT: ‚úÖ CERTIFI√â POUR UTILISATION\n")
            f.write("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
            f.write("‚Ä¢ Impl√©mentation math√©matiquement correcte\n")
            f.write("‚Ä¢ Convergence d'ordre 2 parfaitement valid√©e\n")
            f.write("‚Ä¢ Stabilit√© num√©rique exceptionnelle\n")
            f.write("‚Ä¢ Robustesse face aux cas limites confirm√©e\n")
            f.write("‚Ä¢ Gestion d'erreurs s√©curis√©e et compl√®te\n")
            f.write("‚Ä¢ Performance computationnelle optimale\n\n")
            
            f.write("PROCHAINES √âTAPES RECOMMAND√âES:\n")
            f.write("‚úì Int√©gration en environnement de production\n")
            f.write("‚úì Extension aux diff√©rences finies 2D\n")
            f.write("‚úì Documentation utilisateur finale\n")
            f.write("‚úì Optimisations avanc√©es (optionnel)\n")
            
        elif self.analysis['success_rate'] >= 95:
            f.write("‚ö° VALIDATION QUASI-COMPL√àTE - ACCEPTABLE\n\n")
            f.write("STATUT: ‚úÖ UTILISABLE AVEC R√âSERVES MINEURES\n")
            f.write("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
            f.write("‚Ä¢ Impl√©mentation globalement correcte\n")
            f.write("‚Ä¢ √âchecs mineurs probablement li√©s √†:\n")
            f.write("  - Pr√©cision machine pour cas extr√™mes\n")
            f.write("  - Tol√©rances de tests tr√®s strictes\n")
            f.write("  - Cas limites th√©oriques acceptables\n\n")
            
            f.write("ACTIONS RECOMMAND√âES:\n")
            f.write("‚Ä¢ Analyser les √©checs mineurs (voir section √âchecs)\n")
            f.write("‚Ä¢ Ajuster les tol√©rances si n√©cessaire\n")
            f.write("‚Ä¢ Validation sur cas d'usage r√©els\n")
            f.write("‚Ä¢ Proc√©der aux √©tapes suivantes du projet\n")
            
        else:
            f.write("‚ö†Ô∏è CORRECTIONS N√âCESSAIRES\n\n")
            f.write("STATUT: ‚ùå R√âVISION REQUISE AVANT UTILISATION\n")
            f.write("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
            f.write(f"‚Ä¢ {self.stats['tests']['failed']} test(s) critique(s) √©chou√©(s)\n")
            f.write("‚Ä¢ R√©vision de l'impl√©mentation n√©cessaire\n\n")
            
            f.write("ACTIONS PRIORITAIRES:\n")
            f.write("‚Ä¢ Analyser chaque √©chec d√©taill√©\n")
            f.write("‚Ä¢ Corriger les erreurs d'impl√©mentation\n")
            f.write("‚Ä¢ Relancer la validation compl√®te\n")
            f.write("‚Ä¢ V√©rifier la logique math√©matique\n")
    
    def _ecrire_annexes_txt(self, f):
        """√âcrit les annexes techniques"""
        f.write(f"\n‚ñì‚ñì‚ñì ANNEXES TECHNIQUES ‚ñì‚ñì‚ñì\n")
        f.write("=" * 60 + "\n")
        
        # Informations d'ex√©cution
        f.write("ENVIRONNEMENT D'EX√âCUTION\n")
        f.write("-" * 30 + "\n")
        f.write(f"Syst√®me d'exploitation: {os.name}\n")
        f.write(f"Python: {sys.version.split()[0]}\n")
        f.write(f"R√©pertoire de travail: {os.getcwd()}\n")
        f.write(f"Commande ex√©cut√©e: {self.raw_results.get('command', 'N/A')}\n")
        f.write(f"Heure de d√©but: {self.stats['execution']['start_time']}\n")
        f.write(f"Heure de fin: {self.stats['execution']['end_time']}\n\n")
        
        # Couverture d√©taill√©e
        f.write("COUVERTURE DES TESTS\n")
        f.write("-" * 30 + "\n")
        f.write(f"Cat√©gories couvertes: {', '.join(self.stats['coverage']['categories_tested'])}\n")
        f.write(f"Compl√©tude: {self.stats['coverage']['completeness']:.1f}%\n\n")
        
        # M√©tadonn√©es du rapport
        f.write("M√âTADONN√âES DU RAPPORT\n")
        f.write("-" * 30 + "\n")
        f.write(f"G√©n√©r√© le: {self.metadata['date']}\n")
        f.write(f"Version du rapport: {self.metadata['version']}\n")
        f.write(f"Auteur: {self.metadata['auteur']}\n")
        
        # Pied de page
        f.write("\n" + "‚ñà" * 100 + "\n")
        f.write(f"Rapport g√©n√©r√© automatiquement par TestReporterProfessionnel v{self.metadata['version']}\n")
        f.write(f"Date de g√©n√©ration: {self.metadata['date']}\n")
        f.write(f"Auteur du projet: {self.metadata['auteur']} - {self.metadata['institution']}\n")
        f.write("‚ñà" * 100 + "\n")
    
    def generer_rapport_markdown_professionnel(self):
        """G√©n√®re un rapport Markdown de niveau professionnel"""
        
        filename = f"rapport_professionnel_DF1D_{self.metadata['timestamp']}.md"
        filepath = os.path.join(self.rapport_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8', newline='') as f:
                self._ecrire_entete_md(f)
                self._ecrire_resume_md(f)
                self._ecrire_resultats_md(f)
                self._ecrire_analyse_md(f)
                self._ecrire_details_md(f)
                self._ecrire_conclusions_md(f)
                self._ecrire_annexes_md(f)
            
            print(f"‚úÖ Rapport Markdown g√©n√©r√©: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration rapport Markdown: {e}")
            return None
    
    def _ecrire_entete_md(self, f):
        """√âcrit l'en-t√™te Markdown avec badges"""
        f.write("# üß™ Rapport de Validation Professionnel\n")
        f.write("## Diff√©rences Finies 1D - Analyse Compl√®te\n\n")
        
        # Badges de statut
        if self.analysis['production_ready']:
            status_badge = "![Status](https://img.shields.io/badge/Status-VALIDATED-brightgreen)"
            quality_badge = "![Quality](https://img.shields.io/badge/Quality-PRODUCTION_READY-brightgreen)"
        elif self.analysis['success_rate'] >= 95:
            status_badge = "![Status](https://img.shields.io/badge/Status-QUASI_COMPLETE-green)"
            quality_badge = "![Quality](https://img.shields.io/badge/Quality-ACCEPTABLE-green)"
        else:
            status_badge = "![Status](https://img.shields.io/badge/Status-NEEDS_REVIEW-orange)"
            quality_badge = "![Quality](https://img.shields.io/badge/Quality-REVISION_REQUIRED-orange)"
        
        coverage_badge = f"![Coverage](https://img.shields.io/badge/Coverage-{self.stats['tests']['passed']}/{self.stats['tests']['total']}-blue)"
        method_badge = "![Method](https://img.shields.io/badge/Method-Finite_Differences-purple)"
        order_badge = "![Order](https://img.shields.io/badge/Convergence-O(h¬≤)-yellow)"
        time_badge = f"![Time](https://img.shields.io/badge/Execution-{self.stats['execution']['duration']:.2f}s-lightgrey)"
        
        f.write(f"{status_badge} {quality_badge} {coverage_badge}\n")
        f.write(f"{method_badge} {order_badge} {time_badge}\n\n")
        
        # Table des m√©tadonn√©es
        f.write("## üìã Informations du Projet\n\n")
        f.write("| Propri√©t√© | Valeur |\n")
        f.write("|-----------|--------|\n")
        f.write(f"| **Projet** | {self.metadata['projet']} |\n")
        f.write(f"| **Auteur** | {self.metadata['auteur']} |\n")
        f.write(f"| **Institution** | {self.metadata['institution']} |\n")
        f.write(f"| **Cours** | {self.metadata['cours']} |\n")
        f.write(f"| **Date** | {self.metadata['date']} |\n")
        f.write(f"| **Version** | {self.metadata['version']} |\n")
        f.write(f"| **M√©thode** | {self.metadata['methode']} |\n")
        f.write(f"| **√âquation** | `{self.metadata['equation']}` |\n\n")
    
    def _ecrire_resume_md(self, f):
        """√âcrit le r√©sum√© ex√©cutif Markdown"""
        f.write("## üìä R√©sum√© Ex√©cutif\n\n")
        
        # Table des r√©sultats principaux
        f.write("### üéØ R√©sultats Principaux\n\n")
        f.write("| M√©trique | Valeur | √âvaluation |\n")
        f.write("|----------|--------|------------|\n")
        
        # Statut global
        if self.analysis['production_ready']:
            status_icon = "‚úÖ"
            status_text = "VALIDATION COMPL√àTE"
            eval_text = "üöÄ Production Ready"
        elif self.analysis['success_rate'] >= 95:
            status_icon = "‚ö°"
            status_text = "QUASI-VALIDATION"
            eval_text = "‚úÖ Acceptable"
        else:
            status_icon = "‚ö†Ô∏è"
            status_text = "R√âVISION REQUISE"
            eval_text = "üîß √Ä corriger"
        
        f.write(f"| **Statut Global** | {status_icon} {status_text} | {eval_text} |\n")
        f.write(f"| **Tests Ex√©cut√©s** | {self.stats['tests']['total']} | ‚ÑπÔ∏è Total |\n")
        f.write(f"| **Succ√®s** | {self.stats['tests']['passed']} | ‚úÖ Valid√©s |\n")
        f.write(f"| **√âchecs** | {self.stats['tests']['failed']} | {'‚ùå Critiques' if self.stats['tests']['failed'] > 1 else '‚ö†Ô∏è Mineurs' if self.stats['tests']['failed'] == 1 else '‚úÖ Aucun'} |\n")
        f.write(f"| **Taux de R√©ussite** | {self.analysis['success_rate']:.1f}% | {'üéØ Excellent' if self.analysis['success_rate'] >= 95 else 'üëç Bon' if self.analysis['success_rate'] >= 80 else '‚ö†Ô∏è Insuffisant'} |\n")
        f.write(f"| **Temps d'Ex√©cution** | {self.stats['execution']['duration']:.2f}s | {'‚ö° Rapide' if self.stats['execution']['duration'] < 5 else 'üëç Acceptable' if self.stats['execution']['duration'] < 20 else 'üêå Lent'} |\n")
        f.write(f"| **Couverture** | {self.stats['coverage']['completeness']:.1f}% | {'üéØ Compl√®te' if self.stats['coverage']['completeness'] >= 90 else 'üëç Bonne' if self.stats['coverage']['completeness'] >= 70 else '‚ö†Ô∏è Partielle'} |\n\n")
        
        # Recommandation principale
        f.write("### üí° Recommandation Principale\n\n")
        if self.analysis['production_ready']:
            f.write("üöÄ **VALIDATION R√âUSSIE** - Le solver est **certifi√© pour utilisation en production**.\n\n")
            f.write("‚ú® L'impl√©mentation des diff√©rences finies 1D est math√©matiquement correcte, ")
            f.write("num√©riquement stable et robuste. Tous les crit√®res de validation sont satisfaits.\n\n")
        elif self.analysis['success_rate'] >= 95:
            f.write("‚ö° **QUASI-VALIDATION** - Le solver est **acceptable avec r√©serves mineures**.\n\n")
            f.write("‚úÖ L'impl√©mentation est globalement correcte. Les √©checs mineurs sont probablement ")
            f.write("li√©s √† des cas limites acceptables ou √† la pr√©cision machine.\n\n")
        else:
            f.write("‚ö†Ô∏è **R√âVISION N√âCESSAIRE** - Des corrections sont requises avant utilisation.\n\n")
            f.write("üîß Analyser les √©checs d√©taill√©s et corriger les probl√®mes identifi√©s avant validation finale.\n\n")
    
    def _ecrire_resultats_md(self, f):
        """√âcrit les r√©sultats d√©taill√©s Markdown"""
        f.write("## üìà R√©sultats par Cat√©gorie\n\n")
        
        # Analyse par cat√©gorie
        categories_stats = {}
        for test in self.stats['tests']['details']:
            cat = test['category']
            if cat not in categories_stats:
                categories_stats[cat] = {'total': 0, 'passed': 0, 'failed': 0, 'tests': []}
            
            categories_stats[cat]['total'] += 1
            categories_stats[cat]['tests'].append(test)
            if test['status'] == 'PASSED':
                categories_stats[cat]['passed'] += 1
            elif test['status'] == 'FAILED':
                categories_stats[cat]['failed'] += 1
        
        # Table de r√©sum√© par cat√©gorie
        f.write("### üìä R√©sum√© par Cat√©gorie\n\n")
        f.write("| Cat√©gorie | Tests | Succ√®s | √âchecs | Taux | Statut |\n")
        f.write("|-----------|-------|--------|--------|------|--------|\n")
        
        category_icons = {
            'base': 'üéØ',
            'limite': '‚ö°',
            'fonction': 'üîß',
            'conditions': 'üéõÔ∏è',
            'robustesse': 'üõ°Ô∏è',
            'convergence': 'üìà',
            'combine': 'üé™',
            'performance': '‚ö°',
            'autre': '‚ùì'
        }
        
        for category, stats in sorted(categories_stats.items()):
            icon = category_icons.get(category, 'üìã')
            success_rate = (stats['passed'] / max(stats['total'], 1)) * 100
            
            if success_rate == 100:
                status = "‚úÖ Parfait"
            elif success_rate >= 90:
                status = "üü¢ Excellent"
            elif success_rate >= 80:
                status = "üü° Bon"
            else:
                status = "üî¥ Probl√®me"
            
            f.write(f"| {icon} {category.capitalize()} | {stats['total']} | {stats['passed']} | {stats['failed']} | {success_rate:.1f}% | {status} |\n")
        
        f.write("\n")
        
        # D√©tails par cat√©gorie avec tests collaps√©s
        f.write("### üìù D√©tails par Cat√©gorie\n\n")
        
        for category, stats in sorted(categories_stats.items()):
            icon = category_icons.get(category, 'üìã')
            category_name = category.capitalize()
            
            f.write(f"#### {icon} {category_name}\n\n")
            f.write(f"**R√©sum√©**: {stats['passed']}/{stats['total']} tests r√©ussis ")
            f.write(f"({(stats['passed']/max(stats['total'],1)*100):.1f}%)\n\n")
            
            # Liste des tests
            f.write("<details>\n")
            f.write(f"<summary>Voir les {stats['total']} tests de cette cat√©gorie</summary>\n\n")
            f.write("| Test | Statut |\n")
            f.write("|------|--------|\n")
            
            for test in stats['tests']:
                status_emoji = {
                    'PASSED': '‚úÖ',
                    'FAILED': '‚ùå',
                    'ERROR': 'üí•',
                    'SKIPPED': '‚è≠Ô∏è'
                }.get(test['status'], '‚ùì')
                
                test_name = test['name']
                if len(test_name) > 60:
                    test_name = test_name[:57] + "..."
                
                f.write(f"| `{test_name}` | {status_emoji} {test['status']} |\n")
            
            f.write("\n</details>\n\n")
    
    def _ecrire_analyse_md(self, f):
        """√âcrit l'analyse technique Markdown"""
        f.write("## üî¨ Analyse Technique\n\n")
        
        if self.analysis['production_ready']:
            f.write("### ‚úÖ Validation Math√©matique Compl√®te\n\n")
            f.write("üéØ **Tous les crit√®res de validation sont satisfaits**\n\n")
            f.write("#### Points Valid√©s\n\n")
            f.write("- ‚úÖ **Convergence O(h¬≤)** : Ordre th√©orique parfaitement confirm√©\n")
            f.write("- ‚úÖ **Stabilit√© num√©rique** : Aucune instabilit√© d√©tect√©e\n")
            f.write("- ‚úÖ **Pr√©cision maximale** : Pr√©cision machine atteinte pour les cas exacts\n")
            f.write("- ‚úÖ **Robustesse** : Tous les cas limites et pathologiques g√©r√©s\n")
            f.write("- ‚úÖ **S√©curit√©** : Gestion d'erreurs compl√®te et s√©curis√©e\n")
            f.write("- ‚úÖ **Performance** : Temps d'ex√©cution optimal\n\n")
            
        elif self.analysis['success_rate'] >= 95:
            f.write("### ‚ö° Validation Quasi-Compl√®te\n\n")
            f.write("üéØ **Validation globalement r√©ussie avec r√©serves mineures**\n\n")
            f.write("#### Points Valid√©s\n\n")
            f.write("- ‚úÖ **Convergence O(h¬≤)** : Largement confirm√©e\n")
            f.write("- ‚úÖ **Stabilit√© num√©rique** : Tr√®s bonne\n")
            f.write("- ‚úÖ **Robustesse** : Excellente sur la majorit√© des cas\n")
            f.write("- ‚ö†Ô∏è **√âchecs mineurs** : Probablement li√©s √† des cas limites acceptables\n\n")
            
            f.write("#### Analyse des √âchecs Mineurs\n\n")
            f.write("Les √©checs d√©tect√©s sont vraisemblablement dus √† :\n")
            f.write("- üî¨ **Pr√©cision machine** : Pour les solutions exactes (polyn√¥mes degr√© ‚â§ 2)\n")
            f.write("- üìê **Tol√©rances strictes** : Crit√®res de tests tr√®s rigoureux\n")
            f.write("- üéØ **Cas limites th√©oriques** : Comportements aux bornes acceptables\n\n")
            
        else:
            f.write("### ‚ö†Ô∏è Validation Partielle\n\n")
            f.write("üîß **Corrections n√©cessaires avant validation compl√®te**\n\n")
            f.write(f"- ‚ùå **{self.stats['tests']['failed']} test(s) critique(s) √©chou√©(s)**\n")
            f.write(f"- üí• **{self.stats['tests']['errors']} erreur(s) technique(s)**\n")
            f.write("- üîç **Analyse d√©taill√©e requise** (voir section √âchecs)\n\n")
        
        # M√©triques de qualit√©
        f.write("### üìä M√©triques de Qualit√©\n\n")
        f.write("| Aspect | √âvaluation | Score | Commentaire |\n")
        f.write("|--------|------------|-------|-------------|\n")
        
        # √âvaluation de la couverture
        if self.stats['coverage']['completeness'] >= 90:
            cov_eval = "üéØ Excellente"
            cov_score = "A+"
        elif self.stats['coverage']['completeness'] >= 70:
            cov_eval = "üëç Bonne"
            cov_score = "B+"
        else:
            cov_eval = "‚ö†Ô∏è Partielle"
            cov_score = "C"
        
        f.write(f"| Couverture de tests | {cov_eval} | {cov_score} | {self.stats['coverage']['completeness']:.1f}% des cat√©gories |\n")
        
        # √âvaluation de la robustesse
        robustesse_rate = 100  # Par d√©faut
        if 'robustesse' in [test['category'] for test in self.stats['tests']['details']]:
            robustesse_tests = [t for t in self.stats['tests']['details'] if t['category'] == 'robustesse']
            robustesse_passed = sum(1 for t in robustesse_tests if t['status'] == 'PASSED')
            robustesse_rate = (robustesse_passed / max(len(robustesse_tests), 1)) * 100
        
        if robustesse_rate >= 95:
            rob_eval = "üõ°Ô∏è Excellente"
            rob_score = "A+"
        elif robustesse_rate >= 80:
            rob_eval = "üëç Bonne"
            rob_score = "B+"
        else:
            rob_eval = "‚ö†Ô∏è √Ä am√©liorer"
            rob_score = "C"
        
        f.write(f"| Robustesse | {rob_eval} | {rob_score} | {robustesse_rate:.1f}% de r√©ussite |\n")
        
        # √âvaluation de la performance
        if self.stats['execution']['duration'] < 2:
            perf_eval = "‚ö° Excellente"
            perf_score = "A+"
        elif self.stats['execution']['duration'] < 10:
            perf_eval = "üëç Bonne"
            perf_score = "B+"
        else:
            perf_eval = "üêå Acceptable"
            perf_score = "C"
        
        f.write(f"| Performance | {perf_eval} | {perf_score} | {self.stats['execution']['duration']:.2f}s total |\n")
        
        # √âvaluation globale
        if self.analysis['success_rate'] >= 95:
            global_eval = "üèÜ Excellente"
            global_score = "A+"
        elif self.analysis['success_rate'] >= 80:
            global_eval = "üëç Bonne"
            global_score = "B+"
        else:
            global_eval = "‚ö†Ô∏è √Ä am√©liorer"
            global_score = "C"
        
        f.write(f"| Qualit√© Globale | {global_eval} | {global_score} | {self.analysis['success_rate']:.1f}% de r√©ussite |\n\n")
    
    def _ecrire_details_md(self, f):
        """√âcrit les d√©tails techniques Markdown"""
        if self.stats['failures']['count'] > 0:
            f.write("## üîç Analyse des √âchecs\n\n")
            f.write(f"**{self.stats['failures']['count']} √©chec(s) d√©tect√©(s)**\n\n")
            
            for i, failure in enumerate(self.stats['failures']['details'], 1):
                f.write(f"### ‚ùå √âchec #{i}: `{failure['test']}`\n\n")
                f.write(f"**Cat√©gorie**: {failure['category']}\n\n")
                f.write(f"**Assertion**: `{failure['assertion']}`\n\n")
                
                f.write("<details>\n")
                f.write("<summary>Voir les d√©tails techniques</summary>\n\n")
                f.write("```\n")
                f.write(failure['full_detail'][:1000])  # Limite √† 1000 caract√®res
                if len(failure['full_detail']) > 1000:
                    f.write("\n... (tronqu√©)")
                f.write("\n```\n")
                f.write("</details>\n\n")
        
        # Analyse des performances
        if self.stats['performance']['slowest_tests']:
            f.write("## ‚ö° Analyse des Performances\n\n")
            f.write(f"**Temps total**: {self.stats['execution']['duration']:.3f}s\n")
            f.write(f"**Temps moyen par test**: {self.stats['performance']['average_duration']:.4f}s\n\n")
            
            f.write("### üêå Tests les plus lents\n\n")
            f.write("| Test | Dur√©e |\n")
            f.write("|------|-------|\n")
            
            for test in self.stats['performance']['slowest_tests'][:3]:
                test_name = test['test']
                if len(test_name) > 50:
                    test_name = test_name[:47] + "..."
                f.write(f"| `{test_name}` | {test['duration']:.3f}s |\n")
            
            f.write("\n")
    
    def _ecrire_conclusions_md(self, f):
        """√âcrit les conclusions Markdown"""
        f.write("## üéØ Conclusions et Recommandations\n\n")
        
        if self.analysis['production_ready']:
            f.write("### üöÄ Validation Compl√®te - Production Ready\n\n")
            f.write("‚ú® **Le solver de diff√©rences finies 1D est enti√®rement valid√©** et certifi√© pour utilisation.\n\n")
            f.write("#### üéâ Accomplissements\n\n")
            f.write("- üéØ **Impl√©mentation parfaite** des diff√©rences finies centr√©es d'ordre 2\n")
            f.write("- üìê **Convergence O(h¬≤)** math√©matiquement confirm√©e\n")
            f.write("- üõ°Ô∏è **Robustesse exceptionnelle** face √† tous les cas test√©s\n")
            f.write("- ‚ö° **Performance optimale** pour l'usage pr√©vu\n")
            f.write("- üîí **S√©curit√©** et gestion d'erreurs compl√®tes\n\n")
            f.write("#### üõ§Ô∏è Prochaines √âtapes Recommand√©es\n\n")
            f.write("1. ‚úÖ **D√©ploiement en production** - Le code est pr√™t\n")
            f.write("2. üöÄ **Extension aux diff√©rences finies 2D** - √âtape suivante naturelle\n")
            f.write("3. üìö **Documentation utilisateur** - Finaliser la documentation\n")
            f.write("4. üîß **Optimisations avanc√©es** - Am√©liorer les performances (optionnel)\n")
            f.write("5. üß™ **Tests d'int√©gration** - Validation sur cas d'usage r√©els\n\n")
            
        elif self.analysis['success_rate'] >= 95:
            f.write("### ‚ö° Validation Quasi-Compl√®te - Acceptable\n\n")
            f.write("üéØ **Le solver est globalement valid√©** avec des r√©serves mineures acceptables.\n\n")
            f.write("#### üéâ Points Forts\n\n")
            f.write("- ‚úÖ **Impl√©mentation correcte** des diff√©rences finies\n")
            f.write("- ‚úÖ **Convergence largement confirm√©e** sur la majorit√© des cas\n")
            f.write("- ‚úÖ **Robustesse excellente** pour les cas principaux\n")
            f.write("- ‚ö†Ô∏è **√âchecs mineurs** probablement acceptables\n\n")
            
            f.write("#### üîç Actions Recommand√©es\n\n")
            f.write("1. üî¨ **Analyser les √©checs mineurs** - V√©rifier s'ils sont acceptables\n")
            f.write("2. üìê **Ajuster les tol√©rances** - Si n√©cessaire pour les cas limites\n")
            f.write("3. ‚úÖ **Proc√©der aux √©tapes suivantes** - Le solver est utilisable\n")
            f.write("4. üìù **Documentation des limitations** - Noter les cas limites\n\n")
            
        else:
            f.write("### ‚ö†Ô∏è R√©vision N√©cessaire\n\n")
            f.write("üîß **Des corrections sont requises** avant la validation finale.\n\n")
            f.write("#### üîç Probl√®mes Identifi√©s\n\n")
            f.write(f"- ‚ùå **{self.stats['tests']['failed']} test(s) critique(s) √©chou√©(s)**\n")
            f.write(f"- üí• **{self.stats['tests']['errors']} erreur(s) technique(s)**\n")
            f.write("- üìä **Taux de r√©ussite insuffisant** pour validation\n\n")
            
            f.write("#### üõ†Ô∏è Plan d'Action\n\n")
            f.write("1. üîç **Analyser chaque √©chec** - Identifier les causes racines\n")
            f.write("2. üîß **Corriger l'impl√©mentation** - R√©soudre les probl√®mes identifi√©s\n")
            f.write("3. üß™ **Relancer la validation** - V√©rifier les corrections\n")
            f.write("4. üìö **R√©viser la th√©orie** - Si n√©cessaire pour les aspects math√©matiques\n\n")
        
        # Recommandations g√©n√©rales
        f.write("### üí° Recommandations G√©n√©rales\n\n")
        
        f.write("#### üìö Documentation\n")
        f.write("- Maintenir la documentation technique √† jour\n")
        f.write("- Documenter les cas limites et leurs comportements\n")
        f.write("- Cr√©er des exemples d'usage pour les utilisateurs\n\n")
        
        f.write("#### üî¨ Tests et Validation\n")
        f.write("- Conserver cette suite de tests pour r√©gression\n")
        f.write("- Ajouter des tests pour nouveaux cas d'usage\n")
        f.write("- Valider r√©guli√®rement les performances\n\n")
        
        f.write("#### üöÄ √âvolution Future\n")
        f.write("- Planifier l'extension aux probl√®mes 2D\n")
        f.write("- Consid√©rer des m√©thodes d'ordre sup√©rieur\n")
        f.write("- √âvaluer l'int√©gration avec d'autres solveurs\n\n")
    
    def _ecrire_annexes_md(self, f):
        """√âcrit les annexes techniques Markdown"""
        f.write("## üìé Annexes Techniques\n\n")
        
        # Environnement d'ex√©cution
        f.write("### üñ•Ô∏è Environnement d'Ex√©cution\n\n")
        f.write("| Propri√©t√© | Valeur |\n")
        f.write("|-----------|--------|\n")
        f.write(f"| Syst√®me | {os.name} |\n")
        f.write(f"| Python | {sys.version.split()[0]} |\n")
        f.write(f"| R√©pertoire | `{os.getcwd()}` |\n")
        f.write(f"| Commande | `{self.raw_results.get('command', 'N/A')}` |\n")
        f.write(f"| D√©but | {self.stats['execution']['start_time']} |\n")
        f.write(f"| Fin | {self.stats['execution']['end_time']} |\n")
        f.write(f"| Dur√©e | {self.stats['execution']['duration']:.3f}s |\n\n")
        
        # Statistiques d√©taill√©es
        f.write("### üìä Statistiques D√©taill√©es\n\n")
        f.write("#### Tests par Statut\n")
        f.write("```\n")
        f.write(f"‚úÖ PASSED : {self.stats['tests']['passed']:>3}\n")
        f.write(f"‚ùå FAILED : {self.stats['tests']['failed']:>3}\n")
        f.write(f"üí• ERROR  : {self.stats['tests']['errors']:>3}\n")
        f.write(f"‚è≠Ô∏è SKIPPED: {self.stats['tests']['skipped']:>3}\n")
        f.write(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
        f.write(f"üìä TOTAL  : {self.stats['tests']['total']:>3}\n")
        f.write("```\n\n")
        
        # Couverture par cat√©gorie
        f.write("#### Couverture par Cat√©gorie\n")
        f.write("```\n")
        for cat in sorted(self.stats['coverage']['categories_tested']):
            f.write(f"üìã {cat.capitalize():<12}: ‚úÖ Couverte\n")
        f.write(f"\nüìà Compl√©tude: {self.stats['coverage']['completeness']:.1f}%\n")
        f.write("```\n\n")
        
        # M√©tadonn√©es du rapport
        f.write("### üìÑ M√©tadonn√©es du Rapport\n\n")
        f.write("| Propri√©t√© | Valeur |\n")
        f.write("|-----------|--------|\n")
        f.write(f"| Version | {self.metadata['version']} |\n")
        f.write(f"| G√©n√©r√© le | {self.metadata['date']} |\n")
        f.write(f"| Framework | {self.metadata['framework']} |\n")
        f.write(f"| Auteur | {self.metadata['auteur']} |\n")
        f.write(f"| Institution | {self.metadata['institution']} |\n\n")
        
        # Pied de page
        f.write("---\n\n")
        f.write(f"*üìä Rapport g√©n√©r√© automatiquement par TestReporterProfessionnel v{self.metadata['version']}*  \n")
        f.write(f"*üìÖ {self.metadata['date']} - {self.metadata['auteur']} - {self.metadata['institution']}*  \n")
        f.write(f"*üè´ {self.metadata['cours']} - {self.metadata['projet']}*\n")
    
    def generer_rapports_complets(self):
        """
        Point d'entr√©e principal pour g√©n√©ration compl√®te des rapports
        """
        print("=" * 80)
        print("üìä G√âN√âRATEUR DE RAPPORTS PROFESSIONNELS")
        print("=" * 80)
        print(f"üéØ Projet: {self.metadata['projet']}")
        print(f"üë§ Auteur: {self.metadata['auteur']}")
        print(f"üìÖ Date: {self.metadata['date']}")
        print("=" * 80)
        
        try:
            # √âtape 1: Ex√©cution des tests
            print("\nüöÄ √âTAPE 1: EX√âCUTION DES TESTS")
            if not self.executer_tests_robuste():
                print("‚ö†Ô∏è Probl√®me lors de l'ex√©cution, mais on continue l'analyse...")
            
            # √âtape 2: Analyse des r√©sultats
            print("\nüìà √âTAPE 2: ANALYSE DES R√âSULTATS")
            self.analyser_resultats_complet()
            
            # √âtape 3: G√©n√©ration des rapports
            print("\nüìù √âTAPE 3: G√âN√âRATION DES RAPPORTS")
            rapport_txt = self.generer_rapport_txt_professionnel()
            rapport_md = self.generer_rapport_markdown_professionnel()
            
            # Rapport final
            print("\n" + "=" * 80)
            print("‚úÖ G√âN√âRATION TERMIN√âE AVEC SUCC√àS")
            print("=" * 80)
            
            if rapport_txt:
                print(f"üìÑ **Rapport TXT**: {rapport_txt}")
            if rapport_md:
                print(f"üìù **Rapport MD**: {rapport_md}")
            
            print(f"üìÅ **Dossier**: {self.rapport_dir}")
            print("=" * 80)
            
            # R√©sum√© final
            if self.analysis.get('production_ready', False):
                print("üéâ **STATUT**: ‚úÖ VALIDATION COMPL√àTE R√âUSSIE")
                print("üöÄ **Le solver est certifi√© pour utilisation !**")
            elif self.analysis.get('success_rate', 0) >= 95:
                print("‚ö° **STATUT**: ‚úÖ VALIDATION QUASI-COMPL√àTE")
                print("üëç **Le solver est acceptable avec r√©serves mineures**")
            else:
                print("‚ö†Ô∏è **STATUT**: üîß R√âVISION N√âCESSAIRE")
                print("üìã **Consulter les rapports pour les d√©tails**")
            
            print(f"üìä **Tests**: {self.stats['tests']['passed']}/{self.stats['tests']['total']} r√©ussis")
            print(f"‚è±Ô∏è **Dur√©e**: {self.stats['execution']['duration']:.2f}s")
            print("=" * 80)
            
            return self.raw_results.get('success', False)
            
        except Exception as e:
            print(f"\n‚ùå ERREUR FATALE LORS DE LA G√âN√âRATION")
            print(f"üí• {e}")
            print(f"üîç Traceback: {traceback.format_exc()}")
            return False


def main():
    """
    Point d'entr√©e principal du g√©n√©rateur de rapports
    """
    print("üéØ G√âN√âRATEUR DE RAPPORTS PROFESSIONNELS")
    print("üìã Utilisation: G√©n√©ration de rapports de validation complets")
    print("-" * 60)
    
    try:
        reporter = TestReporterProfessionnel()
        success = reporter.generer_rapports_complets()
        return 0 if success else 1
        
    except Exception as e:
        print(f"üí• Erreur fatale: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())